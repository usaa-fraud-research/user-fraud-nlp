Fraud Detection â€¢ Semantic Search â€¢ Streamlit Dashboard â€¢ Supabase Vector DB

This project analyzes CFPB (Consumer Financial Protection Bureau) articles to detect fraud patterns, compute embeddings, and power a semantic search engine using Supabase pgvector + OpenAI.
A Streamlit dashboard displays the full pipeline.

â¸»

ğŸ“ Project Structure
user-fraud-nlp/
â”‚
â”œâ”€â”€ cfpb_articles.py          # Scraper for CFPB newsroom/blog/enforcement
â”œâ”€â”€ articles_supabase.py      # Upload scraped data to Supabase
â”œâ”€â”€ llm_embedding.py          # Generate & store embeddings in Supabase
â”œâ”€â”€ semantic_search.py        # Cached semantic search using pgvector
â”œâ”€â”€ fraud_dashboard.py        # Streamlit app (Scraper â€¢ Fraud Detection â€¢ Analysis)
â”‚
â”œâ”€â”€ data/                     # (ignored) scraped CSVs
â”œâ”€â”€ .env                      # (ignored) secrets
â”œâ”€â”€ .gitignore                # ignore env, data, pycache, venv
â”œâ”€â”€ pyproject.toml            # dependencies (uv)
â””â”€â”€ uv.lock

ğŸš€ Pipeline Overview

1ï¸âƒ£ CFPB Scraper

Scrapes articles from:
	â€¢	CFPB Newsroom
	â€¢	CFPB Blog
	â€¢	Enforcement Actions

Extracts:
title, date, url, text, source

2ï¸âƒ£ Fraud Detection

Each article is tagged using regex-based patterns:

Examples:
	â€¢	identity theft
	â€¢	account takeover
	â€¢	card fraud
	â€¢	wire/Zelle fraud
	â€¢	phishing / smishing / vishing
	â€¢	crypto fraud
	â€¢	romance scams
	â€¢	loan/investment/insurance/job scams

Outputs:
	â€¢	fraud_type
	â€¢	fraud_tags
	â€¢	summary

â¸»

3ï¸âƒ£ Supabase Storage

Tables used:
	â€¢	cfpb_articles (fraud metadata + embeddings)
	â€¢	search_queries (cached query embeddings)

â¸»

4ï¸âƒ£ Embeddings Pipeline

Uses OpenAIâ€™s text-embedding-3-small (1536-dim) to embed article text.

5ï¸âƒ£ Semantic Search Engine

Built with:
	â€¢	Query embedding (cached)
	â€¢	Supabase RPC + pgvector
	â€¢	Cosine similarity
	â€¢	Optional filters: year, keyword, threshold

6ï¸âƒ£ Streamlit Dashboard

All results visualized in fraud_dashboard.py:

Tabs:
	â€¢	Week 2: Scraper viewer
	â€¢	Week 3: Fraud keywords, charts, word cloud
	â€¢	Week 4: Trend analysis + Semantic Search

ğŸ” Environment Setup

Create .env:
SUPABASE_URL=your_url
SUPABASE_KEY=your_key
OPENAI_API_KEY=your_key

ğŸ§¹ .gitignore
.env
data/
__pycache__/
*.pyc
.venv/