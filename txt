1. Overview of the Model

We trained a Logistic Regression classifier using:
	â€¢	X (features):
1536-dimensional OpenAI text embeddings of article text.
	â€¢	y (labels):
The fraud category (fraud_type) assigned by our rule-based tagging system.

The goal of the model is to predict the fraud type of a new article based on its vector embedding.

â¸»

2. Dataset Summary

After scraping ~75 articles and cleaning the data:
	â€¢	We dropped labels that had fewer than 2 samples, because the libaray cannot stratify train/test splits with single-example classes.
	â€¢	Dropped: loan_servicing, student_loan
	â€¢	Final dataset:
73 total samples
1536 embedding features (vector dimension)
8 fraud labels remaining

This dataset size is extremely small for multi-class classification, and the classes are imbalanced.

â¸»

3. Test Results

The test accuracy is:
Accuracy: ~40%

The classification report shows that:
	â€¢	generic and fcra have the best performance.
	â€¢	Many labels (e.g., identity_theft, reg_z, reg_e, udap) show precision = 0, recall = 0.

This happens when the model never predicts those labels on the test set.

â¸»

4. Why This Happened

ðŸ”¸ A) Very Small Dataset

Machine learning models need hundreds to thousands of samples per class for strong performance.

We have:
	â€¢	73 samples total
	â€¢	8 labels
	â€¢	Some labels have only 2â€“3 samples

This is not enough for the model to learn meaningful patterns.

â¸»

ðŸ”¸ B) Imbalanced Classes

Some fraud types appear much more often than others.

The model learns to predict the most common labels (generic, fcra) and ignores the rare ones.

â¸»

ðŸ”¸ C) No Hyperparameter Tuning

We used a baseline Logistic Regression with default settings.

This is fine for a demo, but itâ€™s not optimized.

â¸»

5. How to Interpret the Results

This model is a proof-of-concept only.

What you can truthfully say in your project:

We trained a multi-class fraud classification model using OpenAI embeddings and Logistic Regression.
Due to the small and imbalanced dataset (73 articles across 8 classes), the model performs well on common categories like generic and fcra, but struggles with rare categories.
The model demonstrates how embeddings can be used as numeric features for downstream machine learning tasks, but is not yet suitable for production fraud detection.
For this project, the classifier is used mainly to produce preliminary risk scores, validate feature importance, and demonstrate an end-to-end ML pipeline.

â¸»

6. Next Steps (Optional Future Work)

If we want better performance, we should:
	â€¢	Collect more labeled data (100â€“200+ per class)
	â€¢	Reduce the number of fraud types (merge rare labels)
	â€¢	Try a more powerful classifier (XGBoost, SVM, or fine-tuned BERT)
	â€¢	Perform hyperparameter tuning
	â€¢	Use class balancing or class weighting
